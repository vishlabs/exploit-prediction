# Author: Visshwanth Reddy
# Script to build and analyse prediction models

import pandas as pd
import warnings                        # To ignore any warnings

# Loading Tools
from sklearn import model_selection 
from sklearn.preprocessing import LabelEncoder

# Loading Classification Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

warnings.filterwarnings("ignore")

##############################################
# Data - Loading, Exploring, Cleansing
##############################################
# Loading data
nvd = pd.read_csv('nvd_data.csv')
cvedetails = pd.read_csv('cvedetails_data.csv')

# Merging data from nvd & cvedetails
data = pd.merge(nvd, cvedetails, on='ID', how='outer')
data.reset_index(drop=True, inplace=True)

# Exploring data
data.shape
data.columns
data.dtypes
data.describe(include="all")
data.isnull().sum()

# Removing rows with no baseScore 
data.drop(data[data['BS'].isnull()].index.tolist(), inplace=True)
data.reset_index(drop=True, inplace=True)

# Removing rows with no VALID value
data.drop(data[data['VALID'].isnull()].index.tolist(), inplace=True)

# Removing rows with no VALID == 0 value
data.drop(data[data['VALID'] == 0].index.tolist(), inplace=True)
data.reset_index(drop=True, inplace=True)
data.shape
data.isnull().sum()

# Exploring every column of the data
# Data Description
# Variable Description             : Possible Values
# ====================             = ================
# Access Vector (AV)               : LOCAL/NETWORK/ADJACENT_NETWORK
# Access Complexity (AC)           : LOW/MEDIUM/HIGH
# Authentication (A)               : NONE/SINGLE/MULTIPLE
# Confidentiality Impact (CI)      : COMPLETE/PARTIAL/NONE
# Integrity Impact (II)            : COMPLETE/PARTIAL/NONE
# Availability Impact (AI)         : COMPLETE/PARTIAL/NONE
# Base Score (BS)                  : 0 - 10
# Severity (S)                     : LOW/MEDIUM/HIGH
# Exploitability Score (ES)        : 0 - 10
# Impact Score (IS)                : 0 - 10
# Obtain All Privilege (OAP)       : True/False
# Obtain User Privilege (OUP)      : True/False
# Obtain Other Privilege (OOP)     : True/False
# User Interaction Required (UIR)  : True/False
#
# Additional Enrichment 'features' added to improve the model
# Enrichment Variable Description  : Possible Values
# ===============================  = ================
# Vulnerability Type               : Brief categorization (190 possibilities)
# Common Weakness Enumeration (CWE): CWE Code (107 possibilities)
# OVAL Defination Present          : True/False
# Impacted Asset Category          : Hardware/Operating System/Application
#
# Target Variable		   : Possible Values
# =============== 		   = ================
# Exploit Available (E)		   : True/False


# Filling-in Missing values
data.loc[data['VULN_TYPE'].isnull().tolist(),'VULN_TYPE'] = 'None'

dataMaster = data.copy()

# Evaluating base dataset with no enrichment features

# Encoding columns
oneHotList = ['AV', 'AC', 'A', 'CI', 'II', 'AI', 'S', 'ES', 'IS'] 
encodeList = ['BS', 'OAP', 'OUP', 'OOP', 'UIR'] 
dropList = ['ID', 'VEN', 'DESC', 'VALID', 'PUB_EXPLOIT', 'VULN_TYPE','CWE', 'OVAL', 'HW', 'OS', 'APP', 'NP', 'PRODUCT_COUNT','VENDORS','METASPLOIT']

# One Hot Encoding
data=pd.get_dummies(data, prefix=oneHotList, columns=oneHotList)

# Encoding
encoder = LabelEncoder()
for each in encodeList:
	data[each] = encoder.fit_transform(data[each])

# Dropping unwanted columns
data.drop(dropList, inplace=True, axis=1)
data.reset_index(drop=True, inplace=True)

# Creating dependent vector (X) and target (y)
X = data.copy()
X.drop('E', axis=1, inplace=True)
y = data['E']

##############################################
models = {}
algos = []
algos.append(('Logistic Regression', LogisticRegression()))
algos.append(('KNeighbors',  KNeighborsClassifier()))
algos.append(('Naive Bayes',  GaussianNB()))
algos.append(('Decision Tree',  DecisionTreeClassifier()))
algos.append(('Random Forest',  RandomForestClassifier()))
algos.append(('Gradient Boosting',  GradientBoostingClassifier()))
models['Classification'] = algos

# Evaluating each model in turn
scoring = 'accuracy'		
analysisType = 'Classification'
results = []
names = []

print '\nBase data set accuracy:'
for name, model in models[analysisType]:
        kfold = model_selection.KFold(n_splits=10, random_state=7)
        cvResults = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)
        results.append(cvResults)
        names.append(name)
        msg = "%s: %f (%f)" % (name, cvResults.mean(), cvResults.std())
        print(msg)

# Evaluating base dataset with no enrichment features
data = dataMaster.copy()

# Encoding columns
oneHotList = ['AV', 'AC', 'A', 'CI', 'II', 'AI', 'S', 'ES', 'IS']
encodeList = ['BS', 'OAP', 'OUP', 'OOP', 'UIR', 'VULN_TYPE','CWE', 'OVAL', 'HW', 'OS', 'APP', 'NP']
dropList = ['ID', 'VEN', 'DESC', 'VALID', 'PUB_EXPLOIT', 'PRODUCT_COUNT','VENDORS','METASPLOIT']

# One Hot Encoding
data=pd.get_dummies(data, prefix=oneHotList, columns=oneHotList)

# Encoding
encoder = LabelEncoder()
for each in encodeList:
	data[each] = encoder.fit_transform(data[each])

# Dropping unwanted columns
data.drop(dropList, inplace=True, axis=1)
data.reset_index(drop=True, inplace=True)

# Creating dependent vector (X) and target (y)
X = data.copy()
X.drop('E', axis=1, inplace=True)
y = data['E']

##############################################
models = {}
algos = []
algos.append(('Logistic Regression', LogisticRegression()))
algos.append(('KNeighbors',  KNeighborsClassifier()))
algos.append(('Naive Bayes',  GaussianNB()))
algos.append(('Decision Tree',  DecisionTreeClassifier()))
algos.append(('Random Forest',  RandomForestClassifier()))
algos.append(('Gradient Boosting',  GradientBoostingClassifier()))
models['Classification'] = algos

# Evaluating each model in turn
scoring = 'accuracy'		
analysisType = 'Classification'
results = []
names = []

print '\nBase + enrichment data set accuracy:'
for name, model in models[analysisType]:
        kfold = model_selection.KFold(n_splits=10, random_state=7)
        cvResults = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)
        results.append(cvResults)
        names.append(name)
        msg = "%s: %f (%f)" % (name, cvResults.mean(), cvResults.std())
        print(msg)

