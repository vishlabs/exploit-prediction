# Author: Visshwanth Reddy
# Script to download enrichment dataset and create a CSV file

import requests
import pickle
import os
import multiprocessing as mp
import pandas as pd
import bs4 as bs
import re
import glob

# Function for reading a pickle file
def ReadPickle(fileName):
	with open(fileName,"rb") as f:
		data = pickle.load(f)
	return data

# Function for saving a pickle file
def SavePickle(data, fileName):
	with open(fileName,"wb") as f:
		pickle.dump(data,f)

# Function to manage multiple processes
def ManageProcess(procs):
	# Start processes
	for p in procs:
    		p.start()
	# Exit the completed processes
	for p in procs:
    		p.join()

# Function for creating a unique list
def UniqueList(data):
	d = {}
	for each in data:
		d[each] = 1
	rData = sorted(d.keys())
	return rData

# Function for fetching specified url
def FetchURL(url):
	headers = {'User-Agent': 'Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.27 Safari/537.17'}
	try:
		response = requests.get(url, headers=headers)
	except:
		print 'Error fetching page:', url
		response = ''
	return response

# Function for splitting a given list into 'x' parts
def SplitList(dataList, x):
	data = {}
	listLen = len(dataList)/x
	for i in range(0, x):
		if x-1 == i:
			data[i] = dataList[i*listLen:]
		else:
			data[i] = dataList[i*listLen:(i+1)*listLen]
	return data

# Function for removing \t & \n
def CleanString(string):
	string = re.sub('\t+','',string)
	string = re.sub('\n+','',string)
	return string

# Function for extracting data from html table 
def ExtractDataFromTable(dataTable, colID):
	data = {}
	rows = dataTable.findAll('tr')
	for each in rows[1:]:
		cols = each.findAll('td')
		if len(cols) > 2:
			data[cols[colID].text] = 1
		else:
			data['None'] = 1
	data = sorted(ExtractInfo(data))
	return data

# Function for Extracting data from dictionary keys
def ExtractInfo(data):
	rData = []
	for each in data.keys():
		each = CleanString(each)
		rData.append(each)
	return rData

# Function for extracting data
def ExtractData(response, ID):
	soup = bs.BeautifulSoup(response.text, 'lxml')
	valid = 0
	publicExploit, vulnTyp, cwe, oval = 0, 0, 0, 0
	hw, os, app, np = 0, 0, 0, 0
	productCnt, vendorList, metasploit = 0, 0, 0
	# Extracting 'h2' count to see if CVE is valid
	h2s = soup.findAll('h2')
	if len(h2s) != 0:
		valid = 1
		# Extracting Public Exploit info #
		vData = soup.find('h1').text
		if vData.find('public exploit') > 0:
			publicExploit = 1
		# Extracting data from 'cvssscorestable'
		dataTable = soup.find('table', {'id':'cvssscorestable'})
		rows = dataTable.findAll('tr')
		vulnTyp = CleanString(rows[7].find('td').text)
		cwe = rows[8].find('td').text
		# Checking to see if OVAL Definitions exist
		for h2 in h2s:
			if h2.text.find('OVAL') > 0:
				oval = 1
		# Extracting Affetcted product Types
		dataTable = soup.find('table', {'id':'vulnprodstable'})
		productType = ExtractDataFromTable(dataTable, 1)
		for typ in productType:
			if typ == 'Application':
				app = 1
			if typ == 'Hardware':
				hw = 1
			if typ == 'OS':
				os = 1
			if typ == 'No Product':
				np = 1
		# Extracting Affected Product Count & Vendor Names
		dataTable = soup.find('table', {'id':'vulnversconuttable'})
		if len(dir(dataTable)) > 16: 
			vendorList = ExtractDataFromTable(dataTable, 0)
			productCnt = len(vendorList)
			vendorList = '-'.join(UniqueList(vendorList))
		# Extracting Metasploit info
		dataTable = soup.find('table', {'class':'metasploit'})
		if len(dir(dataTable)) > 20:
			metasploit = 1
	
	data = {'ID':ID,'VALID':valid,'PUB_EXPLOIT':publicExploit,'VULN_TYPE':vulnTyp,'CWE':cwe, 'OVAL':oval,'HW':hw,'OS':os,'APP':app,'NP':np,'PRODUCT_COUNT':productCnt,'VENDORS':vendorList, 'METASPLOIT':metasploit}
	return data

# Function for fetching specified url and extracting data
def FetchList(dataList, urlFormat, dataDir, pid):
	pFileName = dataDir + '/cvedetails_' + str(pid) + '.pickle'
	cveData = []
	responseData = {}
	for i, cve in enumerate(dataList):
		url = '{}/{}'.format(urlFormat, cve)
		print pid, 'Fetching: #', i, url
		response = FetchURL(url)
		if response != '':
			responseData[cve] = response
			data = ExtractData(response, cve)
			cveData.append(data)
	df = pd.DataFrame(cveData)
	cols = ['ID','VALID','PUB_EXPLOIT','VULN_TYPE','CWE','OVAL','HW','OS','APP','NP','PRODUCT_COUNT','VENDORS', 'METASPLOIT']
	df = df[cols]
	df.to_csv('{}/cvedetails_{}.csv'.format(dataDir, pid), index=False, encoding='utf-8')
	SavePickle(data, pFileName)

# Function for merging csv files
def MergeCSVFiles(dataDir):
	fileList = glob.glob('{}/cvedetails*.csv'.format(dataDir))
	df = []
	for csvFile in fileList:
		df.append(pd.read_csv(csvFile))
	
	data = pd.concat(df)
	data.reset_index(inplace=True)
	cols = ['ID','VALID','PUB_EXPLOIT','VULN_TYPE','CWE','OVAL','HW','OS','APP','NP','PRODUCT_COUNT','VENDORS', 'METASPLOIT']
	data = data[cols]
	data.to_csv('cvedetails_data.csv', index=False, encoding='utf-8')

################################################################################
# Main #
dataDir = 'data'
urlFormat = 'https://www.cvedetails.com/cve'
processCnt = 20
processes = []

cve = pd.read_csv('nvd_data.csv')
cveList = SplitList(cve['ID'].tolist(), processCnt)

for i in range(0, processCnt):
	processes.append(mp.Process(target=FetchList, args=(cveList[i],urlFormat,dataDir,i, )))

ManageProcess(processes)
MergeCSVFiles(dataDir)

